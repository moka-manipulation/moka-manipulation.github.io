<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting">
  <meta name="keywords" content="Visual Prompting, GPT4V">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MOKA</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kuanfang.github.io">Kuan Fang*</a></span>,
            <span class="author-block">
              <a href="https://fangchenliu.github.io">Fangchen Liu*</a></span>,
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup> denotes equal contribution, alphabetical order</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Berkeley AI Research, UC Berkeley</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rail-berkeley/moka"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.gif"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
      <h2 class="content has-text-justified">
        <b>MOKA</b> employs pre-trained vision-language models (GPT-4V) to predict <b>point-based affordance representations</b> for solving open-vocabulary manipulation tasks. By <b>annotating marks</b> (candidate points, grids, and captions) on RGB images, MOKA converts the motion generation problem into a series of visual question-answering problems that the VLM can tackle.
      </h2>
    </div>
  </div>
</section>


<!--<section class="hero is-light is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-steve">-->
<!--          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/steve.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-chair-tp">-->
<!--          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/chair-tp.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-shiba">-->
<!--          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/shiba.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-fullbody">-->
<!--          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/fullbody.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-blueshirt">-->
<!--          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/blueshirt.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-mask">-->
<!--          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/mask.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-coffee">-->
<!--          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/coffee.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-toby">-->
<!--          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/toby2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
            Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse envi- ronments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present Marking Open-vocabulary Keypoint Affordances (MOKA), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language de- scriptions. At the heart of our approach is a compact point- based representation of affordance and motion that bridges the VLM’s predictions on RGB images and the robot’s motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM’s reasoning in zero-shot, we propose a visual prompting technique that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of visual question answering problems that are feasible for the VLM to solve. Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through in-context learning and policy distillation. We evaluate and analyze MOKA’s performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement.
            <!--   We present MOKA, an approach that employs VLMs' reasoning capabilities for robotic manipulation in open-vocabulary settings. -->
            <!--   At the heart of our approach is a compact point-based representation of affordance and motion that bridges -->
            <!--   the VLM's predictions on RGB images and the robot's motions in the physical world. -->
            <!-- [> </p> <] -->
            <!-- [> <p> <] -->
            <!--   By prompting a VLM pre-trained on internet-scale data, our approach predicts the affordances and generates the corresponding motions -->
            <!--   by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM's reasoning in zero-shot, -->
            <!--   we propose a visual prompting technique that annotates visual and textual marks on the raw images, converting the prediction -->
            <!--   of keypoints and waypoints into a visual question answering problem that is feasible for the VLM to solve. -->
            <!-- [> </p> <] -->
            <!-- [> <p> <] -->
            <!--   We evaluate and analyze MOKA's performance across a wide variety of manipulation tasks specified by -->
            <!--   free-form language descriptions, spanning across tool use, deformable body manipulation, object rearrangement, etc. -->
            <!--   MOKA can successfully achieve high success rates on all these tasks in zero-shot or few-shot and -->
            <!--   provide interpretable intermediate results for human users. -->
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

  <div class="container is-fullhd">
    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Marking Open-vocabulary Keypoint Affordances (MOKA)</h2>
          <div>
            <img src="./static/images/model.png"
                       class="interpolation-image"
                       alt="Interpolation end reference image."/>
          </div>
          <h2 class="content has-text-justified">
            MOKA first queries VLM to to decompose a task into multiple one-step subtasks, which are defined by possible involved objects and desired motions.
            Then it leverages the open-vocabulary vision model to extract object keypoints from segmentation masks. For each subtask, we further query VLM to select keypoints
            and waypoints to perform the desired motion.
          </h2>
        </div>
      </div>
    </div>

    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Point-based Affordance Representations</h2>
          <h2 class="content has-text-justified">
            To bridge the VLM’s predictions on 2D images and the robot’s motion in the physical world, we introduce a point-based affordance representation. Using a set of keypoints and waypoints, we can specify the robot’s motions for a wide range of tasks. 
          </h2>
          <div>
          <img src="./static/images/affordances.png"
                     class="interpolation-image"
                     alt="Interpolation end reference image."
                     width="800px"
                     />
          </div>
        </div>
      </div>
    </div>

    <!-- <div class="container is-max-widescreen"> -->
    <div class="container is-fullhd">

      <!-- <div class="rows"> -->
      <!-- <div class="container is-fullhd"> -->
      <div class="rows is-fullhd">
        <!-- <div class="column has-text-centered"> -->
        <h2 class="title is-3">Tasks</h2>
        
        <p class="content has-text-justified">
          Given free-form descriptions of the tasks, MOKA can effectively predict the point-based affordance representations and generates the desired motions.
        </p>
        
        <div class="rows">
          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="media/videos/.mp4" type="video/mp4">
              </video>
              <p style="text-align:center">
                ""
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="media/videos/.mp4" type="video/mp4">
              </video>
              <p style="text-align:center">
                ""
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="media/videos/.mp4" type="video/mp4">
              </video>
              <p style="text-align:center">
                ""
              </p>
            </div>
          </div>

          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="media/videos/.mp4" type="video/mp4">
              </video>
              <p style="text-align:center">
                ""
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="media/videos/.mp4" type="video/mp4">
              </video>
              <p style="text-align:center">
                ""
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="media/videos/.mp4" type="video/mp4">
              </video>
              <p style="text-align:center">
                ""
              </p>
            </div>
          </div>

          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="media/videos/.mp4" type="video/mp4">
              </video>
              <p style="text-align:center">
                ""
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="media/videos/.mp4" type="video/mp4">
              </video>
              <p style="text-align:center">
                ""
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="media/videos/.mp4" type="video/mp4">
              </video>
              <p style="text-align:center">
                ""
              </p>
            </div>
          </div>

        </div>

      </div>
    </div>

    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Robustness to variances.</h2>
          <div>
          <h2 class="content has-text-justified">
            For the same task, MOKA's prediction is robust to variations of instructions, objects, and initial poses. Each column in the image uses the same language instruction and similar initial arrangements of objects. The two rows involve different objects.
          </h2>
          <img src="./static/images/robustness.png"
                     class="interpolation-image"
                     alt="Interpolation end reference image."
                     width="800px"
                     />
          </div>
        </div>
      </div>
    </div>

    <!-- <div class="my-block"> -->
    <!--   <div class="columns is-centered has-text-centered"> -->
    <!--     <div class="column is-four-fifths"> -->
    <!--       <h2 class="title is-3">Tasks</h2> -->
    <!--       <div> -->
    <!--         <img src="./static/images/tasks.png" class="interpolation-image" alt="Interpolation end reference image."/> -->
    <!--       </div> -->
    <!--       <h2 class="content has-text-justified"> -->
    <!--       <span class="dnerf">MOKA</span> is evaluated on a wide variety of manipulation tasks specified by free-form language descriptions, spanning across tool use, deformable body manipulation, object rearrangement, etc. -->
    <!--     </h2> -->
    <!--     </div> -->
    <!--   </div> -->
    <!-- </div> -->
    <!--  -->
    <!-- <div class="my-block"> -->
    <!--   <div class="columns is-centered has-text-centered"> -->
    <!--     <div class="column is-full-width"> -->
    <!--       <h2 class="title is-3">Examples</h2> -->
    <!--       <div> -->
    <!--       <img src="./static/images/examples.jpg" -->
    <!--                  class="interpolation-image" -->
    <!--                  alt="Interpolation end reference image."/> -->
    <!--       </div> -->
    <!--       <h2 class="content has-text-centered">The illustration examples of visual prompts, VLM's responses and robot execution results on the evaluation tasks we proposed. -->
    <!--     </h2> -->
    <!--     </div> -->
    <!--   </div> -->
    <!-- </div> -->


    <!-- Paper video. -->
<!--    TODO: replace video -->
  <!--   <div class="columns is-centered has-text-centered"> -->
  <!--     <div class="column is-four-fifths"> -->
  <!--       <h2 class="title is-3">Video</h2> -->
  <!--       <div class="publication-video"> -->
  <!--         <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" -->
  <!--                 frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
  <!--       </div> -->
  <!--       <h2 class="content has-text-centered"> -->
  <!--       For more results and details about <span class="dnerf">MOKA</span>, please check the videos. -->
  <!--     </h2> -->
  <!--     </div> -->
  <!--   </div> -->
  <!--   [>/ Paper video. <] -->
  <!-- </div> -->

</section>



<!--<section class="section" id="BibTeX">-->
<!-- TODO: update with arxiv bibtex -->
<!--  <div class="container is-max-desktop content">-->
<!--    <h2 class="title">BibTeX</h2>-->
<!--    <pre><code>@article{park2021nerfies,-->
<!--  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},-->
<!--  title     = {Nerfies: Deformable Neural Radiance Fields},-->
<!--  journal   = {ICCV},-->
<!--  year      = {2021},-->
<!--}</code></pre>-->
<!--  </div>-->
<!--</section>-->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website was adapted from nerfie's <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
